{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Second assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement the following functions in the code of the Numpy-based neural network described in the exercises:\n",
    "\n",
    "1. Momentum\n",
    "2. L1 and L2 regularization\n",
    "\n",
    "Put the new parts of the source code between the comments “# HF2 start procedure” and “# HF2 end procedure” (procedure = momentum, l1reg, l2reg) and write comments about exactly what you did and why.\n",
    "\n",
    "Try to keep the code as short as possible!\n",
    "\n",
    "I wrote the following source code based upon the relevant exercise. The modifications are well commented."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch, train_err: 0.2416, valid_err: 0.2196\n",
      "1 epoch, train_err: 0.1955, valid_err: 0.1593\n",
      "2 epoch, train_err: 0.1291, valid_err: 0.1043\n",
      "3 epoch, train_err: 0.0862, valid_err: 0.0788\n",
      "4 epoch, train_err: 0.0666, valid_err: 0.0667\n",
      "5 epoch, train_err: 0.0567, valid_err: 0.0600\n",
      "6 epoch, train_err: 0.0510, valid_err: 0.0556\n",
      "7 epoch, train_err: 0.0473, valid_err: 0.0525\n",
      "8 epoch, train_err: 0.0447, valid_err: 0.0503\n",
      "9 epoch, train_err: 0.0427, valid_err: 0.0485\n",
      "10 epoch, train_err: 0.0412, valid_err: 0.0471\n",
      "11 epoch, train_err: 0.0400, valid_err: 0.0460\n",
      "12 epoch, train_err: 0.0390, valid_err: 0.0450\n",
      "13 epoch, train_err: 0.0382, valid_err: 0.0442\n",
      "14 epoch, train_err: 0.0375, valid_err: 0.0435\n",
      "15 epoch, train_err: 0.0368, valid_err: 0.0430\n",
      "16 epoch, train_err: 0.0363, valid_err: 0.0424\n",
      "17 epoch, train_err: 0.0359, valid_err: 0.0420\n",
      "18 epoch, train_err: 0.0355, valid_err: 0.0416\n",
      "19 epoch, train_err: 0.0351, valid_err: 0.0413\n",
      "\n",
      "--- TESTING ---\n",
      "\n",
      "0 [-0.49551261 -0.7444607 ] 0.13  (required result: 0.00)\n",
      "1 [ 0.89288499 -1.26981044] 0.98  (required result: 1.00)\n",
      "2 [-0.17687074  1.45002022] 0.84  (required result: 1.00)\n",
      "3 [0.23518118 1.54853674] 0.33  (required result: 0.00)\n",
      "4 [-0.91921004 -1.15158256] 0.02  (required result: 0.00)\n",
      "5 [ 0.45007887 -0.45520451] 0.87  (required result: 1.00)\n",
      "6 [-0.7232066   0.52937545] 0.96  (required result: 1.00)\n",
      "7 [0.52408431 0.74905089] 0.09  (required result: 0.00)\n",
      "8 [-1.13124799 -0.64346812] 0.05  (required result: 0.00)\n",
      "9 [ 1.05619852 -1.16867333] 0.99  (required result: 1.00)\n",
      "10 [-1.64331284  0.32178486] 0.86  (required result: 1.00)\n",
      "11 [1.27592446 0.47490425] 0.07  (required result: 0.00)\n",
      "12 [-0.85234509 -1.67766207] 0.03  (required result: 0.00)\n",
      "13 [ 1.06439281 -0.01519469] 0.60  (required result: 1.00)\n",
      "14 [-1.49598073  1.1721386 ] 0.99  (required result: 1.00)\n",
      "15 [1.00867052 1.33834844] 0.01  (required result: 0.00)\n",
      "16 [-0.96073952 -0.75856217] 0.04  (required result: 0.00)\n",
      "17 [ 0.96820285 -0.62312564] 0.97  (required result: 1.00)\n",
      "18 [-1.27731201  1.01086655] 0.99  (required result: 1.00)\n",
      "19 [1.24921224 1.45125364] 0.01  (required result: 0.00)\n",
      "20 [-0.68034828 -1.39272014] 0.04  (required result: 0.00)\n",
      "21 [ 0.19203454 -0.69725794] 0.80  (required result: 1.00)\n",
      "22 [-1.35447081  1.3051462 ] 0.99  (required result: 1.00)\n",
      "23 [0.75811891 1.38066171] 0.03  (required result: 0.00)\n",
      "24 [ 0.04865448 -0.90985725] 0.70  (required result: 0.00)\n",
      "25 [ 0.83494361 -0.42420374] 0.94  (required result: 1.00)\n",
      "26 [-0.42625033  1.69214366] 0.90  (required result: 1.00)\n",
      "27 [0.9747302  0.63829576] 0.03  (required result: 0.00)\n",
      "28 [-0.89859695 -1.16324684] 0.02  (required result: 0.00)\n",
      "29 [ 0.99406789 -1.27367092] 0.98  (required result: 1.00)\n",
      "30 [-0.4986541   1.23885647] 0.97  (required result: 1.00)\n",
      "31 [0.94899899 0.93553572] 0.02  (required result: 0.00)\n",
      "32 [-0.98989922 -0.28933404] 0.28  (required result: 0.00)\n",
      "33 [ 1.4178541  -0.81594234] 0.97  (required result: 1.00)\n",
      "34 [-1.28515493  0.39272723] 0.94  (required result: 1.00)\n",
      "35 [1.01544978 0.6359506 ] 0.03  (required result: 0.00)\n",
      "36 [-0.68920096 -1.05628831] 0.04  (required result: 0.00)\n",
      "37 [ 1.23285696 -0.78808995] 0.98  (required result: 1.00)\n",
      "38 [-0.45570044 -0.08445807] 0.50  (required result: 1.00)\n",
      "39 [0.78528024 1.92471042] 0.09  (required result: 0.00)\n",
      "40 [-2.02150252 -0.50132607] 0.22  (required result: 0.00)\n",
      "41 [ 1.07375928 -0.30132133] 0.90  (required result: 1.00)\n",
      "42 [-0.97967981  1.46134931] 0.99  (required result: 1.00)\n",
      "43 [1.43104178 1.01282183] 0.01  (required result: 0.00)\n",
      "44 [-1.42261241 -0.46956705] 0.15  (required result: 0.00)\n",
      "45 [ 0.45148347 -0.9437387 ] 0.95  (required result: 1.00)\n",
      "46 [-0.47857327  1.4169799 ] 0.96  (required result: 1.00)\n",
      "47 [0.73120781 1.20702752] 0.02  (required result: 0.00)\n",
      "48 [-1.59189506 -1.0949691 ] 0.01  (required result: 0.00)\n",
      "49 [ 1.54374629 -0.27527979] 0.75  (required result: 1.00)\n",
      "50 [-0.6027629   0.85996166] 0.98  (required result: 1.00)\n",
      "51 [1.26573007 0.91350389] 0.01  (required result: 0.00)\n",
      "52 [-0.82681195 -0.8418245 ] 0.04  (required result: 0.00)\n",
      "53 [ 0.53999516 -0.55402011] 0.92  (required result: 1.00)\n",
      "54 [-1.38054512  1.27422799] 0.99  (required result: 1.00)\n",
      "55 [0.45722329 0.59802441] 0.17  (required result: 0.00)\n",
      "56 [-2.17252156 -0.95218351] 0.09  (required result: 0.00)\n",
      "57 [ 0.97142945 -0.50835919] 0.96  (required result: 1.00)\n",
      "58 [-0.31580073  0.25126539] 0.72  (required result: 1.00)\n",
      "59 [1.32725243 0.7145116 ] 0.03  (required result: 0.00)\n",
      "60 [-0.93427312 -0.92924414] 0.02  (required result: 0.00)\n",
      "61 [ 0.68579039 -1.28345398] 0.97  (required result: 1.00)\n",
      "62 [-0.7771762   1.29016349] 0.99  (required result: 1.00)\n",
      "63 [0.32342975 0.91520552] 0.18  (required result: 0.00)\n",
      "64 [-1.18335154 -0.38699305] 0.19  (required result: 0.00)\n",
      "65 [ 1.10755805 -0.20065101] 0.83  (required result: 1.00)\n",
      "66 [-1.5946805   1.39085526] 0.99  (required result: 1.00)\n",
      "67 [0.95956766 0.59662601] 0.04  (required result: 0.00)\n",
      "68 [-0.68326656 -0.17477146] 0.41  (required result: 0.00)\n",
      "69 [ 1.60443149 -0.51055056] 0.87  (required result: 1.00)\n",
      "70 [-1.345509    1.06387085] 0.99  (required result: 1.00)\n",
      "71 [1.0399581  1.48467107] 0.01  (required result: 0.00)\n",
      "72 [-0.97578159 -0.8746935 ] 0.02  (required result: 0.00)\n",
      "73 [ 0.59456082 -0.58012958] 0.94  (required result: 1.00)\n",
      "74 [-0.99838184  1.00794573] 0.99  (required result: 1.00)\n",
      "75 [0.67683025 0.94531504] 0.03  (required result: 0.00)\n",
      "76 [-1.38522558 -0.80909313] 0.03  (required result: 0.00)\n",
      "77 [ 0.61733912 -0.56602641] 0.94  (required result: 1.00)\n",
      "78 [-0.07583425  1.30576314] 0.78  (required result: 1.00)\n",
      "79 [0.38597268 0.76179308] 0.16  (required result: 0.00)\n",
      "80 [-1.62192946 -0.87559713] 0.03  (required result: 0.00)\n",
      "81 [ 0.0933582  -1.42405863] 0.74  (required result: 1.00)\n",
      "82 [-0.41178207  1.31268606] 0.96  (required result: 1.00)\n",
      "83 [0.20688708 0.72200264] 0.37  (required result: 0.00)\n",
      "84 [-0.6947673  -0.70454052] 0.07  (required result: 0.00)\n",
      "85 [ 0.15912428 -0.76434734] 0.79  (required result: 1.00)\n",
      "86 [-1.17388859  1.46510224] 0.99  (required result: 1.00)\n",
      "87 [0.79729717 1.08618224] 0.02  (required result: 0.00)\n",
      "88 [-0.95084721 -0.58087122] 0.07  (required result: 0.00)\n",
      "89 [ 0.54332732 -0.58005466] 0.93  (required result: 1.00)\n",
      "90 [-0.36137803  1.75261358] 0.84  (required result: 1.00)\n",
      "91 [1.59511841 0.68196924] 0.05  (required result: 0.00)\n",
      "92 [-0.64393552 -0.51719602] 0.14  (required result: 0.00)\n",
      "93 [ 1.15069897 -0.54158161] 0.96  (required result: 1.00)\n",
      "94 [-0.71245188  0.67784608] 0.97  (required result: 1.00)\n",
      "95 [0.97944428 1.30625314] 0.01  (required result: 0.00)\n",
      "96 [-0.6956089  -1.37664792] 0.04  (required result: 0.00)\n",
      "97 [ 0.49433522 -1.06646624] 0.96  (required result: 1.00)\n",
      "98 [-0.99364269  0.99498732] 0.99  (required result: 1.00)\n",
      "99 [0.44984778 1.28285523] 0.09  (required result: 0.00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dactivation(x):\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        np.random.seed(123)\n",
    "        self.shape = args\n",
    "        n = len(args)\n",
    "        self.layers = []\n",
    "        self.layers.append(np.ones(self.shape[0] + 1))\n",
    "        for i in range(1, n):\n",
    "            self.layers.append(np.ones(self.shape[i]))\n",
    "        self.weights = []\n",
    "        for i in range(n - 1):\n",
    "            self.weights.append(np.zeros((self.layers[i].size, self.layers[i + 1].size)))\n",
    "        self.dw = [0, ] * len(self.weights)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.random.random((self.layers[i].size, self.layers[i + 1].size))\n",
    "            self.weights[i][...] = (2 * Z - 1) * 1\n",
    "\n",
    "    def propagate_forward(self, data):\n",
    "        self.layers[0][0:-1] = data\n",
    "\n",
    "        for i in range(1, len(self.shape)):\n",
    "            self.layers[i][...] = activation(np.dot(self.layers[i - 1], self.weights[i - 1]))\n",
    "\n",
    "        return self.layers[-1]\n",
    "\n",
    "    def propagate_backward(self, target, lrate=0.1):\n",
    "        deltas = []\n",
    "        error = - (target - self.layers[-1])\n",
    "        delta = np.multiply(error, dactivation(np.dot(self.layers[-2], self.weights[-1])))\n",
    "        deltas.append(delta)\n",
    "        for i in range(len(self.shape) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * dactivation(np.dot(self.layers[i - 1], self.weights[i - 1]))\n",
    "            deltas.insert(0, delta)\n",
    "        for i in range(len(self.weights)):\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            delta = np.atleast_2d(deltas[i])\n",
    "            dw = -lrate * np.dot(layer.T, delta)\n",
    "            # HF 2 modification start\n",
    "            # Here you can try out the different methods required in this assignment.\n",
    "            # The current setup uses momentum and L2 regularization.\n",
    "            # I implemented the required functions in a way, that they work with one layer of weights\n",
    "            # (or weights between two neuron-layers...), so I give the 'i' index-variable to each one of them.\n",
    "\n",
    "            # With momentum, I add(!) (based upon the PowerPoint presentation downloaded from the website on 10th October(!))\n",
    "            # the calculated momentum value to dw. (The calculation is based upon the equation seen in the PPT presentation)\n",
    "            # After tryout, I found, that it's the right way to do, because this results in faster convergence.\n",
    "            # If I subtract the momentum value from dw (as I saw on the recent PPT presentation and lecture), the\n",
    "            # convergence of the neural network is actually slower, and the resulting model is worse.\n",
    "            dw += self.momentum(i)\n",
    "\n",
    "            # With L1 and L2 regularization, I subtract(!) the calculated value from dw, as I saw on lecture and on the\n",
    "            # PowerPoint presentation.\n",
    "            dw -= self.l2reg(lrate, i)\n",
    "            # dw -= self.l1reg(lrate, i)\n",
    "            self.weights[i] += dw\n",
    "            self.dw[i] = dw\n",
    "\n",
    "        # If we use L1/L2 regularization, the cost function is modified, based on the PowerPoint presentation.\n",
    "        # So here I add the required summed value to the original cost function value (MSE)\n",
    "        ret = (error ** 2).sum()\n",
    "        # ret += self.l1cost()\n",
    "        ret += self.l2cost()\n",
    "        # HF2 modification end\n",
    "        return ret\n",
    "\n",
    "    # HF2 start momentum\n",
    "    # In this function I calculate the momentum value with the help of the earlier dw values (self.dw[i]).\n",
    "    # The equation is from the PowerPoint presentation seen on lecture.\n",
    "    # I chose 0.5 for the default value of alpha, as I saw on lecture.\n",
    "    def momentum(self, current_index, alpha=0.5):\n",
    "        return alpha * self.dw[current_index]\n",
    "\n",
    "    # HF2 end momentum\n",
    "    # HF2 start l1reg\n",
    "    # In this function I calculate the value needed for the weight modification in the case of L1 regularization\n",
    "    # The equation is from the PowerPoint presentation seen on lecture.\n",
    "    # I chose 0.00001 for the default value of lambda1, as I saw on lecture.\n",
    "    def l1reg(self, lrate, current_index, lambda1=0.00001):\n",
    "        return lrate * lambda1 * np.sign(self.weights[current_index])\n",
    "\n",
    "    # In this function I calculate the value needed for the modified cost function of L1 regularization\n",
    "    # in the return value of propagate_backward() method.\n",
    "    # The equation is from the PowerPoint presentation seen on lecture.\n",
    "    def l1cost(self, lambda1=0.00001):\n",
    "        allw = 0.0\n",
    "        for i in range(len(self.weights)):\n",
    "            allw += np.absolute(self.weights[i]).sum()\n",
    "        return lambda1 * allw\n",
    "\n",
    "    # HF2 end l1reg\n",
    "    # HF2 start l2reg\n",
    "    # In this function I calculate the value needed for the weight modification in the case of L2 regularization\n",
    "    # The equation is from the PowerPoint presentation seen on lecture.\n",
    "    # I chose 0.00001 for the default value of lambda2, as I saw on lecture.\n",
    "    def l2reg(self, lrate, current_index, lambda2=0.00001):\n",
    "        return lrate * lambda2 * self.weights[current_index]\n",
    "\n",
    "    # In this function I calculate the value needed for the modified cost function of L2 regularization\n",
    "    # in the return value of propagate_backward() method.\n",
    "    # The equation is from the PowerPoint presentation seen on lecture.\n",
    "    def l2cost(self, lambda2=0.00001):\n",
    "        allw = 0.0\n",
    "        for i in range(len(self.weights)):\n",
    "            allw += np.power(self.weights[i], 2).sum()\n",
    "        return 0.5 * lambda2 * allw\n",
    "\n",
    "    # HF2 end l2reg\n",
    "\n",
    "\n",
    "def learn(network, X, Y, valid_split, test_split, epochs=20, lrate=0.1):\n",
    "    X_train = X[0:int(nb_samples * (1 - valid_split - test_split))]\n",
    "    Y_train = Y[0:int(nb_samples * (1 - valid_split - test_split))]\n",
    "    X_valid = X[int(nb_samples * (1 - valid_split - test_split)):int(nb_samples * (1 - test_split))]\n",
    "    Y_valid = Y[int(nb_samples * (1 - valid_split - test_split)):int(nb_samples * (1 - test_split))]\n",
    "    X_test = X[int(nb_samples * (1 - test_split)):]\n",
    "    Y_test = Y[int(nb_samples * (1 - test_split)):]\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    randperm = np.random.permutation(len(X_train))\n",
    "    X_train, Y_train = X_train[randperm], Y_train[randperm]\n",
    "\n",
    "    for i in range(epochs):\n",
    "        train_err = 0\n",
    "        for k in range(X_train.shape[0]):\n",
    "            network.propagate_forward(X_train[k])\n",
    "            train_err += network.propagate_backward(Y_train[k], lrate)\n",
    "        train_err /= X_train.shape[0]\n",
    "\n",
    "        valid_err = 0\n",
    "        o_valid = np.zeros(X_valid.shape[0])\n",
    "        for k in range(X_valid.shape[0]):\n",
    "            o_valid[k] = network.propagate_forward(X_valid[k])\n",
    "            valid_err += (o_valid[k] - Y_valid[k]) ** 2\n",
    "        valid_err /= X_valid.shape[0]\n",
    "\n",
    "        print(\"%d epoch, train_err: %.4f, valid_err: %.4f\" % (i, train_err, valid_err))\n",
    "\n",
    "    print(\"\\n--- TESTING ---\\n\")\n",
    "    test_err = 0\n",
    "    o_test = np.zeros(X_test.shape[0])\n",
    "    for k in range(X_test.shape[0]):\n",
    "        o_test[k] = network.propagate_forward(X_test[k])\n",
    "        test_err += (o_test[k] - Y_test[k]) ** 2\n",
    "        print(k, X_test[k], '%.2f' % o_test[k], ' (required result: %.2f)' % Y_test[k])\n",
    "    test_err /= X_test.shape[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nb_samples = 1000\n",
    "\n",
    "    network = MLP(2, 10, 1)\n",
    "\n",
    "    X = np.zeros((nb_samples, 2))\n",
    "    Y = np.zeros(nb_samples)\n",
    "    for i in range(0, nb_samples, 4):\n",
    "        noise = np.random.normal(0, 1, 8)\n",
    "        X[i], Y[i] = (-2 + noise[0], -2 + noise[1]), 0\n",
    "        X[i + 1], Y[i + 1] = (2 + noise[2], -2 + noise[3]), 1\n",
    "        X[i + 2], Y[i + 2] = (-2 + noise[4], 2 + noise[5]), 1\n",
    "        X[i + 3], Y[i + 3] = (2 + noise[6], 2 + noise[7]), 0\n",
    "\n",
    "    network.reset()\n",
    "    learn(network, X, Y, 0.2, 0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}